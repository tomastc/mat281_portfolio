{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/Logo_UTFSM.png\" width=\"200\" alt=\"utfsm-logo\" align=\"left\"/>\n",
    "\n",
    "# MAT281\n",
    "### Aplicaciones de la Matemática en la Ingeniería"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Módulo 04\n",
    "## Clase 02: Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "* Comprender/recordar regresión lineal.\n",
    "* Estimar el error al aplicar modelos matemáticos a los datos.\n",
    "* Introducir la librería `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contenidos\n",
    "* [Regresión Lineal](#linear_regression)\n",
    "* [Implementaciones](#implementations)\n",
    "* [Aplicación](#aplicacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='linear_regression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Técnica universalmente utilizada y a pesar de su simpletaza, la derivación de este método entrega importantes consideraciones sobre su implementación, sus hipótesis y sus posibles extensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Por motivos pedagógicos utilizaremos un set de datos que ha sido testeado a lo largo de los años en la literatura, el que consiste en registros de peso del cerebro y cuerpo de distintos mamíferos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head data/brain_and_body_weight.txt -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'altair'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d7d1edd9f263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'opaque'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Para quienes utilizan temas oscuros en Jupyter Lab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'altair'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "alt.themes.enable('opaque')  # Para quienes utilizan temas oscuros en Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_txt = np.loadtxt(\"data/brain_and_body_weight.txt\", skiprows=33)\n",
    "brain_body_df = pd.DataFrame(data_txt[:, [1, 2]], index=data_txt[:, 0].astype(int), columns=[\"wgt_brain\", \"wgt_body\"])\n",
    "brain_body_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "points = alt.Chart(brain_body_df).mark_circle(size=75, opacity=0.5).encode(\n",
    "    x=\"wgt_brain:Q\",\n",
    "    y=\"wgt_body:Q\"\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Logarithmic Scale\n",
    "log_points = alt.Chart(brain_body_df).mark_circle(size=75, opacity=0.5).encode(\n",
    "    x=alt.X(\"wgt_brain:Q\", \n",
    "          scale=alt.Scale(type=\"log\")),\n",
    "    y=alt.Y(\"wgt_body:Q\", scale=alt.Scale(type=\"log\"))\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "log_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supondremos que tenemos $m$ datos. \n",
    "Cada dato $x^{(i)}$, $i=1,\\dots,$ $m$ tiene $n$ componentes,\n",
    "$x^{(i)} = (x^{(i)}_1, ..., x^{(i)}_n)$. \n",
    "\n",
    "Conocemos además el valor (etiqueta) asociado a $x^{(i)}$ que llamaremos $y^{(i)}$, $i=1,\\dots, m$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nuestra hipótesis de modelo lineal puede escribirse como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h_{\\theta}(x) &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n \\\\\n",
    "          &= \\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n\\end{bmatrix} \\begin{bmatrix}1 \\\\ x_1 \\\\x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\\\\n",
    "          &= \\theta^T \\begin{bmatrix}1\\\\x\\end{bmatrix} = \\begin{bmatrix}1 & x^T\\end{bmatrix} \\theta \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Definiremos $x^{(i)}_0 =1$, de modo que\n",
    "$h_{\\theta}(x^{(i)}) = (x^{(i)})^T \\theta $ y buscamos el vector de parámetros\n",
    "$$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Definamos las matrices\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Y &= \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "1 & x^{(1)}_1 & \\dots & x^{(1)}_n \\\\ \n",
    "1 & x^{(2)}_1 & \\dots & x^{(2)}_n \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x^{(m)}_1 & \\dots & x^{(m)}_n \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix} \n",
    "- (x^{(1)})^T - \\\\ \n",
    "- (x^{(2)})^T - \\\\\n",
    "\\vdots \\\\\n",
    "- (x^{(m)})^T - \\\\\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego la evaluación\n",
    "de todos los datos puede escribirse matricialmente como\n",
    "\n",
    "$$\\begin{aligned}\n",
    "X \\theta &= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & ... & x_n^{(1)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_1^{(m)} & ... & x_n^{(m)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "1 \\theta_0 + x^{(1)}_1 \\theta_1 + ... + x^{(1)}_n \\theta_n \\\\\n",
    "\\vdots \\\\\n",
    "1 \\theta_0 + x^{(m)}_1 \\theta_1 + ... + x^{(m)}_n \\theta_n \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "& = \n",
    "\\begin{bmatrix}\n",
    "h(x^{(1)}) \\\\\n",
    "\\vdots \\\\\n",
    "h(x^{(m)})\n",
    "\\end{bmatrix}\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nuestro problema es\n",
    "encontrar un “buen” conjunto de valores $\\theta$ de modo que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "h(x^{(1)}) \\\\\n",
    "h(x^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "h(x^{(m)})\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}\\end{aligned}$$\n",
    "\n",
    "es decir, que $$X \\theta \\approx Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el mejor vector $\\theta$ podríamos definir una función de costo $J(\\theta)$ de la siguiente manera:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='implementations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aproximación Ingenieril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo podemos resolver el problema\n",
    "en el menor número de pasos?\n",
    "\n",
    "Deseamos resolver el sistema $$A \\theta = b$$ con\n",
    "$A \\in \\mathbb{R}^{m \\times n}$ y $m > n$ (La matrix $A$ es skinny).\n",
    "\n",
    "¿Cómo resolvemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bueno,\n",
    "si $A \\in \\mathbb{R}^{m \\times n}$, entonces\n",
    "$A^T \\in \\mathbb{R}^{n \\times m}$ y la multiplicación está bien definida\n",
    "y obtengo el siguiente sistema lineal, conocido como **Ecuación Normal**:\n",
    "$n \\times n$.\n",
    "$$(A^T A) \\  \\theta = A^T b$$ \n",
    "\n",
    "Si la matriz $A^T A$ es invertible, el sistema se puede solucionar “sin mayor reparo”. $$\\theta = (A^T A)^{-1} A^T b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En nuestro caso, obtendríamos $$\\theta = (X^T X)^{-1} X^T Y$$ Esta\n",
    "respuesta, aunque correcta, no admite interpretaciones y no permite\n",
    "generalizar a otros casos más generales.\n",
    "\n",
    "En particular...\n",
    "\n",
    "-   ¿Qué relación tiene con la función de costo (no) utilizada?\n",
    "\n",
    "-   ¿Qué pasa si $A^T A$ no es invertible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aproximación Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo podemos obtener una\n",
    "buena aproximación para $\\theta$?\n",
    "\n",
    "Queremos encontrar $\\theta^*$ que minimice $J(\\theta)$.\n",
    "\n",
    "Basta con utilizar una buena rutina de optimización para cumplir con\n",
    "dicho objetivo.\n",
    "\n",
    "En particular, una elección natural es tomar la dirección de mayor\n",
    "descenso, es decir, el método del máximo descenso (gradient descent).\n",
    "\n",
    "$$\\theta^{(n+1)} = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)})$$\n",
    "donde $\\alpha >0$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En\n",
    "nuestro caso, puesto que tenemos\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "se tiene que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} &=\n",
    "\\frac{\\partial }{\\partial \\theta_k} \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m}  2 \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\frac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_k}  \\\\\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego, el algoritmo queda como sigue:\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(n+1)} & = \\theta^{(n)} - \\alpha \\nabla_{\\theta} J(\\theta^{(n)}) \\\\\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k}\n",
    "&= \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) x^{(i)}_k\\end{aligned}$$\n",
    "\n",
    "**Observación**: La elección de $\\alpha$ es crucial para la convergencia. En\n",
    "particular, una regla de trabajo es utilizar $0.01/m$. Notar que el parámetro $\\alpha$ no es un parámetro del modelo como tal, si no que es parte del algoritmo, este tipo de parámetros se suelen llamar **hyperparameters**. Pudes reconocerlos porque el valor del parámetro es conocido antes de la fase de entranamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lms_regression_slow(X, Y, theta, tol=1E-6):\n",
    "    m, n = X.shape\n",
    "    converged = False\n",
    "    alpha = 0.01 / len(Y)\n",
    "    while not converged:\n",
    "        gradient = 0.\n",
    "        for xiT, yi in zip(X, Y):\n",
    "            xiT = xiT.reshape(1, n)\n",
    "            hi = np.dot(xiT, theta)\n",
    "            gradient += (hi - yi) * xiT.T\n",
    "        new_theta = theta - alpha * gradient\n",
    "        converged = np.linalg.norm(theta - new_theta) < tol * np.linalg.norm(theta)\n",
    "        theta = new_theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = 1000\n",
    "t = np.linspace(0, 1, m)\n",
    "x = 2 + 2 * t\n",
    "y = 300 + 100 * t\n",
    "X = np.array([np.ones(m), x]).T\n",
    "Y = y.reshape(m, 1)\n",
    "theta_0 = np.array([[0.0], [0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_slow = lms_regression_slow(X, Y, theta_0)\n",
    "print(theta_slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Validamos si nuestro resultado es el indicado con una tolerancia dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.allclose(X @ theta_slow, Y, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.allclose(X @ theta_slow, Y, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementación Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**¿Cómo podemos obtener una justificación para la ecuación normal?**\n",
    "\n",
    "Necesitamos los siguientes ingredientes:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_x &(x^T A x) = A x + A^T x \\\\ \n",
    "\\nabla_x &(b^T x) = b \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Se tiene\n",
    "\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) \n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right) \\\\\n",
    "&= \\frac{1}{2} \\left( X \\theta - Y \\right)^T \\left( X \\theta - Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T - Y^T \\right) \\left( X \\theta - Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T X \\theta - \\theta^T X^T Y - Y^T X \\theta + Y^T Y \\right) \\\\\n",
    "&= \\frac{1}{2} \\left( \\theta^T X^T X \\theta - 2 (Y^T X) \\theta + Y^T Y \\right)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aplicando a cada uno de los términos, obtenemos:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( \\theta^T X^T X \\theta ) &= X^T X \\theta + (X^T X)^T \\theta \\\\\n",
    "& = 2 X^T X \\theta\\end{aligned}$$\n",
    "\n",
    "también se tiene\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( Y^T X \\theta  ) &= (Y^T X) ^T\\\\\n",
    "&= X^T Y\\end{aligned}$$\n",
    "\n",
    "y por último\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta ( Y^T Y  ) = 0\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por lo tanto se tiene que\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "& = \\nabla_\\theta  \\frac{1}{2} \\left( \\theta^T X^T X \\theta - 2 (Y^T X) \\theta + Y^T Y \\right) \\\\\n",
    "&= \\frac{1}{2} ( 2 X^T X \\theta - 2 X^T Y + 0 ) \\\\\n",
    "&= X^T X \\theta - X^T Y \\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto significa que el problema $$\\min_\\theta J(\\theta)$$ se resuelve al\n",
    "hacer todas las derivadas parciales iguales a cero (ie, gradiente igual\n",
    "a cero) $$\\nabla_\\theta J(\\theta) = 0$$ lo cual en nuestro caso se\n",
    "convierte convenientemente a la ecuación normal $$X^T X \\theta = X^T Y$$\n",
    "y se tiene $$\\theta = (X^T X)^{-1} X^T Y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def lms_regression_fast(X, Y, theta, tol=1E-6):\n",
    "    converged = False\n",
    "    alpha = 0.01 / len(Y)\n",
    "    theta = theta.reshape(X.shape[1], 1)\n",
    "    A = np.dot(X.T, X)\n",
    "    b = np.dot(X.T, Y)\n",
    "    while not converged:\n",
    "        gradient = np.dot(A, theta) - b\n",
    "        new_theta = theta - alpha * gradient\n",
    "        converged = np.linalg.norm(theta - new_theta) < tol * np.linalg.norm(theta)\n",
    "        theta = new_theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_fast = lms_regression_fast(X, Y, theta_0)\n",
    "print(theta_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.allclose(X @ theta_slow, Y, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.allclose(X @ theta_fast, Y, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "También es posible usar la implementación de resolución de sistemas lineales dispoinible en numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_regression(X, Y, theta, tol=1E-6):\n",
    "    A = np.dot(X.T,X)\n",
    "    b = np.dot(X.T,Y)\n",
    "    sol = np.linalg.solve(A,b)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta_npsolve = matrix_regression(X, Y, theta_0)\n",
    "print(theta_npsolve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretación Probabilística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Consideremos el modelo lineal\n",
    "\n",
    "$$ Y = X \\theta + \\varepsilon $$\n",
    "\n",
    "donde $\\varepsilon$ es un vector de errores aleatorios de media cero y matriz de dispersión $\\sigma^2 I$, donde $I$ es la matriz identidad. Es usual añadir el supuesto de normalidad al vector de errores, por lo que se asume que \n",
    "\n",
    "$$\\varepsilon \\sim  \\mathcal{N}(0, \\sigma^2 I)$$\n",
    "\n",
    "Cabe destacar que:\n",
    "\n",
    "-   $\\theta$ no es una variable aleatoria, es un parámetro\n",
    "    (desconocido).\n",
    "-   $Y \\ | \\ X; \\theta \\sim \\mathcal{N}(X \\theta, \\sigma^2 I)$\n",
    "\n",
    "\n",
    "La función de verosimilitud $L(\\theta)$ nos\n",
    "permite entender que tan probable es encontrar los datos observados,\n",
    "para una elección del parámetro $\\theta$.\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\left( 2 \\pi \\sigma^2 \\right)^{-n/2} \\, \\exp\\left(- \\frac{1}{2 \\sigma ^2} || Y - X \\theta ||^2 \\right)\n",
    "$$\n",
    "\n",
    "Sea $l(\\theta) = \\log L(\\theta)$ la log-verosimilitud. Luego, ignorando los términos constantes se tiene\n",
    "\n",
    "$$\n",
    "l(\\theta) = -\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2 \\sigma ^2} || Y - X \\theta ||^2\n",
    "$$\n",
    "\n",
    "Luego, derivando respecto a $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta}\n",
    "&= - \\frac{1}{2 \\sigma ^2} \\left( - 2 X^T Y + 2 X^T X \\theta \\right) \\\\\n",
    "&= - \\frac{1}{\\sigma ^2} \\left( X^T Y + X^T X \\theta \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Luego podemos usar toda nuestra artillería de optimización despejando $\\partial l(\\theta) / \\partial \\theta = 0$ y demostrando que es un máximo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Implementación en scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "_**[Scikit-learn](https://scikit-learn.org/)** Machine Learning in Python_\n",
    "\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
    "\n",
    "* Simple and efficient tools for predictive data analysis\n",
    "* Accesible to everybody, and reusable in various contexts\n",
    "* Built on `numpy`, `scipy` and `matplotlib`\n",
    "* Open source, commercially usable - BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Scikit-learn cuenta con enorme cantidad de herramientas de regresión, siendo la regresión lineal la más simple de estas. Ajustar una es tan sencillo como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X, Y)\n",
    "theta_sklearn = reg.coef_.T\n",
    "print(theta_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Nota que primero se crea un objeto `LinearRegression` en que se declaran algunos parámetros, por ejemplo, en nuestro caso la matriz de diseño `X` ya posee una columna de intercepto, por lo que no es necesario incluirla en el modelo de scikit-learn. Luego se ajusta el modelo `reg` con el método `fit()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Implementación simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lms_regression_slow(X, Y, theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Implementación vectorizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lms_regression_fast(X, Y, theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Implementación numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "matrix_regression(X, Y, theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_npsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Implementación scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "LinearRegression(fit_intercept=False).fit(X, Y).coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "theta_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Algunos comentarios:\n",
    "\n",
    "- La implementación simple es **miles de veces** más lenta que la más rápida, que en este caso es la implementación de numpy.\n",
    "- La implementación de numpy es sin duda la más rápida, pero no es posible utilizarla con matrices singulares.\n",
    "- Las implementaciones de _gradient descent_ implementadas _from scratch_ no son lo sufiecientemente precisas.\n",
    "- scikit-learn demora más pues es más flexible, además de realizar validaciones al momento de ajustar los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aspectos Prácticos\n",
    "\n",
    "Al realizar regresión, algunos autores indican que es conveniente normalizar/estandarizar los datos, es\n",
    "decir transformarlos para que tengan una escala común:\n",
    "\n",
    "-   Utilizando la media y la desviación estándar\n",
    "    $$\\frac{x_i-\\overline{x_i}}{\\sigma_{x_i}}$$\n",
    "\n",
    "-   Utilizando mínimos y máximos\n",
    "    $$\\frac{x_i-\\min{x_i}}{\\max{x_i} - \\min{x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**¿Porqué normalizar?**\n",
    "\n",
    "-   Los valores numéricos poseen escalas de magnitud distintas.\n",
    "-   Las variables tienen distintos significados físicos.\n",
    "-   Algoritmos funcionan mejor.\n",
    "-   Interpretación de resultados es más sencilla.\n",
    "\n",
    "**Algunos problemas potenciales**\n",
    "- Normalizar los datos puede producir colinealidad en los datos, produciendo inestabilidad numérica en la implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<a id='application'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_brain = brain_body_df[[\"wgt_brain\"]].values\n",
    "y_brain = brain_body_df[\"wgt_body\"].values\n",
    "brain_reg = LinearRegression(fit_intercept=True).fit(X_brain, y_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reg_df = (\n",
    "    brain_body_df.loc[\n",
    "        lambda x: x[\"wgt_brain\"].isin([x[\"wgt_brain\"].min(), x[\"wgt_brain\"].max()])\n",
    "        , [\"wgt_brain\"]\n",
    "    ].assign(regression=lambda x: brain_reg.intercept_ + x[\"wgt_brain\"] * brain_reg.coef_)\n",
    ")\n",
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reg_line = alt.Chart(reg_df).mark_line(color=\"red\", opacity=0.5).encode(\n",
    "    x=alt.X(\"wgt_brain:Q\"),\n",
    "    y=alt.Y(\"regression:Q\")\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "points + reg_line"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
